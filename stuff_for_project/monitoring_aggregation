import re
import os
import time
import sys
from collections import OrderedDict

log_file_path = str(os.getcwd() + "\\aggregatation_log.txt")  # setting path for a log file. Current is cwd = current working directory.
archivation_folder = str(os.getcwd() + "\\aggregation_logs_archive\\")
archivation_log_file_path = str(archivation_folder + "archivation_log.txt")
log_time = time.ctime(time.time())  # setting a timestamp for the log file. Current is - current local time. You may use time.gmtime() if you like.
max_log_size = 0  # MB


# data_file_path = "/usr/openv/netbackup/bin/PARENT_END_CALLED_MONTH"  # path to data file.


def log_cleanup():
	"""
	Check if log file size is below threshold. If not - archive it and delete original file
	"""
	print("running monitoring aggregation script log archivation")
	print("my execution folder is: " + str(os.getcwd()))
	try:
		os.mkdir(archivation_folder, mode=0o777, )  # trying to create archive folder
	except:
		print("\n" + "Failed to create monitoring aggregation archive directory - already exists")
	log_file_size = (os.path.getsize(log_file_path) // 1024) / 1024  # get log file size in MB
	print("log file size is: " + str(log_file_size) + "MB")
	if log_file_size > int(max_log_size):  # check size
		print("Archivation started")
		print("log file size exceed " + str(max_log_size) + "MB, log file will be arhived, put to \\aggregation_logs_archive\\ folder. Original file will be deleted")
		i = len(os.listdir(path=r"%s" % archivation_folder))  # counter for archive naming
		if os.name == "nt":  # if we're running under windows
			win_arch_log_name = str(archivation_folder + "archived_log" + str(i) + ".tar")  # archive file name
			print("win_arch_log_name  " + win_arch_log_name + "\n" + "log_file_path " + log_file_path)  # debug
			try:
				os.system(r"tar.exe -cf %s %s" % ("\"" + win_arch_log_name + "\"", "\"" + log_file_path + "\""))  # create archive
			except:
				print("archive was not created")
			else:
				with open(archivation_log_file_path, mode="a") as archlog:  # if tar didnt return error - write to log file
					archlog.writelines(str("Archivation done on " + log_time + "\n"))
					archlog.writelines(str("Archive path: " + win_arch_log_name) + "\n" + str("File that was archived: " + log_file_path) + "\n")
				if os.path.exists(win_arch_log_name) and os.path.getsize(win_arch_log_name) > 1000:  # if archive file exists and has size
					os.system(r"del %s" % ("\"" + log_file_path + "\""))
				else:
					print("Log file that had to be created not found ! Supposed name is: " + win_arch_log_name + " Cannot delete it")
		elif os.name == "posix":  # same for linux
			unix_arch_log_name = str(archivation_folder + "archived_log" + str(i) + ".tar")
			try:
				os.system(r"tar -cf %s %s" % ("\"" + unix_arch_log_name + "\"", "\"" + log_file_path + "\""))
			except:
				print("archive was not created")
			else:
				with open(archivation_log_file_path, mode="a") as archlog:
					archlog.writelines(str("Archivation done on " + log_time))
					archlog.writelines(str("Archive path: " + unix_arch_log_name) + "\n" + str("File that was archived: " + log_file_path) + "\n")
				if os.path.exists(unix_arch_log_name) and os.path.getsize(unix_arch_log_name) > 1000:
					os.system(r"rm -f %s" % ("\"" + log_file_path + "\""))
				else:
					print("Log file that had to be created not found ! Supposed name is: " + unix_arch_log_name + " Cannot delete it")
		else:
			print("You are running script on unsupported operating system. Script will be aborted")
			sys.exit()
	else:
		print("Log file size is below threshold, no archivation needed ")

def get_failed_strings(path):
	"""
	:param path: - path to file
	:return strings that doesnt have PROCESSED marker and represent failed jobs:
	"""
	with open(path, mode="r") as file:
		to_process = ""
		for string in file.readlines():
			if "FAILED" in string and "PROCESSED" not in string and "MM" not in string:
				to_process += string
	return to_process


def sort_by_policy(data):
	"""
	:param data: - list of strings(failed backup jobs) that should be processed
	:return: - list grouped by policy - if different failed jobs belong to the same policy -
	these strings are marked and will be joined within single SCOM incident
	"""
	result = []
	log = []
	for item in data.split("\n"):
		if len(item):
			policy = item.split()[7]  # policy to check for
			if len(re.findall(r"\b%s\b" % re.escape(policy), data)) > 1:
				result.append(str(item.rstrip()) + " SortByPolicy_" + str(policy) + " \n")
				# ↑ if we find more than one strings with same policy (we assume that different customers cannot be in same policy) we add "SortByPolicy" to the string
				log.append(
					"\n" + "Log creation time: " + log_time + "\n" + "Following alerts will be aggregated by policy: " + policy + "\n")  # adding policy aggregation header to the log file
				for tmp in data.split("\n"):  # looking for strings that will be aggregated in data
					if policy in tmp:
						log.append(tmp + " \n")  # add them to log
			else:
				result.append(item + "\n")
	with open(log_file_path, mode="a") as logfile:
		for _ in list(OrderedDict.fromkeys(log)):
			logfile.writelines(_)
	return result


def sort_by_code(data):
	"""
	:param data - list of strings processed (or not by sort_by_policy function):
	:return - processed list of strings. Now we look for exit code and customer match - exit code
	aggregation works only within single customer entity:
	"""
	result = []
	log = []
	print(data)
	for item in data:
		if "SortByPolicy" in item:
			result.append(item.rstrip())
		else:
			exit_code = item.split()[9]
			customer = re.split("[-|_]", item.split()[7])[
				0]  # since policies might contain "-" or "_" we need to consider both options
			if len(re.findall(r"\b%s\b" % re.escape(exit_code + " FAILED"), "".join(data))) > 1 and len(
					re.findall(r"\b%s\S*\b\s\S*\s%d\s\S*\s\S*\s\S*\s\d*\s*\n" % (re.escape(customer), int(exit_code)),
					           "".join(data))) > 1:
				result.append(str(item).rstrip() + " SortByExitCode_" + str(exit_code) + "_customer_" + customer)
				# ↑ we look if there are more than 1 same exit codes in data (if not - nothing to aggregate) AND we check if there are more than one(or more - this is modified according to current
				# situation) regular expressions mathing this pattern: customer2-policy2 INCR 2 - we look for "customer2" and "2"(after INCR) so we are sure that we count only same customer with the
				# same exit code.
				log.append(
					"\n" + "Log creation time: " + log_time + "\n" + "Following alerts will be aggregated by exit code: " + exit_code + " for customer: " + customer + "\n")  # adding error code aggregation header to the log file
				for tmp in data:
					if len(re.findall(
							r"\b%s\S*\b\s\S*\s%d\s\S*\s\S*\s\S*\s\d*\s*\n" % (re.escape(customer), int(exit_code)),
							tmp)):  # we look for strings that match the pattern we use to mark strings for error aggregation
						log.append(tmp.rstrip() + "\n")
			else:
				result.append(item.rstrip())
	with open(log_file_path, mode="a") as logfile:  # writing log to the log file
		for _ in list(OrderedDict.fromkeys(log)):
			logfile.writelines(_)

	for _ in result:
		print(_)
	return result


""" TBD:
	1. test archivation
	2. Maybe? extend arch logging so it will know file name + size ?
	3. comments
	4. make initial configuration procedure with config file creation. Detect operating system, determine data file paths
"""

sort_by_code(sort_by_policy(get_failed_strings(r"C:\Users\yaizk\PycharmProjects\coursera_l2\Practice Python\Testing\testing_aggregation.txt")))
log_cleanup()
