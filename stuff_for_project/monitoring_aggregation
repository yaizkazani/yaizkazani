import re
import os
import time
import sys
from collections import OrderedDict

"""Setting variables used by script"""
log_time = time.ctime(time.time())  # setting a timestamp for the log file. Current is - current local time. You may use time.gmtime() if you like.
data_file_path = r"/usr/aggr_test/testing_aggregation.txt"
output_file_path = r""

if os.name == "nt":
	log_file_path = str(os.getcwd() + "\\aggregatation_log.txt")  # setting path for a log file. Current is cwd = current working directory.
	archivation_folder = str(os.getcwd() + "\\aggregation_logs_archive\\")  # path to archivation folder
elif os.name == "posix":
	log_file_path = str(os.getcwd() + "/aggregatation_log.txt")  # LINUX
	archivation_folder = str(os.getcwd() + "/aggregation_logs_archive/")  # path to archivation folder LINUX

archivation_log_file_path = str(archivation_folder + "archivation_log.txt")  # path to archivation log

"""log_cleanup function configuration ↓"""

max_log_size = 20  # arhivation threshold - if log has bigger size it will be archived (MB)
logs = [log_file_path]   # list of logs to be processed by log_cleanup function
############################################################################


def check_env():
	if not os.name == "nt" or os.name == "posix":
		print("You are running script on unsupported operating system. Script will be aborted")
		sys.exit()
	if os.path.exists(data_file_path):
		return True
	else:
		print("Data file not found, path is: ", data_file_path)
		sys.exit()


def log_cleanup(log):
	"""
	Check if log file size is below threshold. If not - archive it and delete original file
	"""
	print("running monitoring aggregation script log archivation")
	print("my execution folder is: " + str(os.getcwd()))
	try:
		os.mkdir(archivation_folder, mode=0o777, )  # trying to create archive folder
	except:
		print("\n" + "Failed to create monitoring aggregation archive directory - already exists")
	log_file_size = (os.path.getsize(log) // 1024) / 1024  # get log file size in MB
	print("log file size is: " + str(log_file_size) + "MB")
	if log_file_size > int(max_log_size):  # check size
		print("Archivation started")
		if os.name == "nt":
			print("log file size exceed " + str(max_log_size) + "MB, log file will be arhived, put to \\aggregation_logs_archive\\ folder. Original file will be deleted")
		else:
			print("log file size exceed " + str(max_log_size) + "MB, log file will be arhived, put to /aggregation_logs_archive/ folder. Original file will be deleted")
		i = len(os.listdir(path=r"%s" % archivation_folder))  # counter for archive naming

		arch_log_name = str(archivation_folder + "archived_log" + str(i) + ".tar")  # archive file name
		print("arch_log_name  " + arch_log_name + "\n" + "log " + log)  # debug
		try:
			os.system(r"tar.exe -cf %s %s" % ("\"" + arch_log_name + "\"", "\"" + log + "\"")) if os.name == "nt" else os.system(r"tar -cf %s %s" % ("\"" + arch_log_name + "\"", "\"" + log + "\""))  # create archive
		except:
			print("archive was not created")
		else:
			with open(archivation_log_file_path, mode="a") as archlog:  # if tar didnt return error - write to log file
				archlog.writelines(str("Archivation done on " + log_time + "\n"))
				archlog.writelines(str("Archive path: " + arch_log_name) + "\n" + str("File that was archived: " + log) + "\n")
			if os.path.exists(arch_log_name) and os.path.getsize(arch_log_name) > 1000:	 # if archive file exists and has size
				if os.name == "nt":
					os.system(r"del %s" % ("\"" + log + "\""))
				elif os.name == "posix":
					os.system(r"rm -f %s" % ("\"" + log + "\""))
			else:
				print("Log file that had to be created not found ! Supposed name is: " + arch_log_name + " Cannot delete it")
	else:
		print("Log file size " + log + " is below threshold, no archivation needed ")


def get_failed_strings(path):
	"""
	:param path: - path to file
	:return strings that doesnt have PROCESSED marker and represent failed jobs:
	"""
	with open(path, mode="r") as file:
		to_process = ""
		for string in file.readlines():
			if "FAILED" in string and "PROCESSED" not in string and "MM" not in string:
				to_process += string
	return to_process


def sort_by_policy(data):
	"""
	:param data: - list of strings(failed backup jobs) that should be processed
	:return: - list grouped by policy - if different failed jobs belong to the same policy -
	these strings are marked and will be joined within single SCOM incident
	"""
	result = []
	log = []
	for item in data.split("\n"):
		if len(item):
			policy = item.split()[7]  # policy to check for
			if len(re.findall(r"\b%s\b" % re.escape(policy), data)) > 1:
				result.append(str(item.rstrip()) + " SortByPolicy_" + str(policy) + " \n")
				# ↑ if we find more than one strings with same policy (we assume that different customers cannot be in same policy) we add "SortByPolicy" to the string
				log.append(
					"\n" + "Log creation time: " + log_time + "\n" + "Following alerts will be aggregated by policy: " + policy + "\n")  # adding policy aggregation header to the log file
				for tmp in data.split("\n"):  # looking for strings that will be aggregated in data
					if policy in tmp:
						log.append(tmp + " \n")  # add them to log
			else:
				result.append(item + "\n")
	with open(log_file_path, mode="a") as logfile:
		for _ in list(OrderedDict.fromkeys(log)):
			logfile.writelines(_)
	return result


def sort_by_code(data):
	"""
	:param data: - list of strings processed (or not by sort_by_policy function)
	:return: - processed list of strings. Now we look for exit code and customer match - exit code
	aggregation works only within single customer entity
	"""
	result = []
	log = []
	print(data)
	for item in data:
		if "SortByPolicy" in item:
			result.append(item.rstrip())
		else:
			exit_code = item.split()[9]
			customer = re.split("[-|_]", item.split()[7])[
				0]  # since policies might contain "-" or "_" we need to consider both options
			if len(re.findall(r"\b%s\b" % re.escape(exit_code + " FAILED"), "".join(data))) > 1 and len(
					re.findall(r"\b%s\S*\b\s\S*\s%d\s\S*\s\S*\s\S*\s\d*\s*\n" % (re.escape(customer), int(exit_code)),
					           "".join(data))) > 1:
				result.append(str(item).rstrip() + " SortByExitCode_" + str(exit_code) + "_customer_" + customer)
				# ↑ we look if there are more than 1 same exit codes in data (if not - nothing to aggregate) AND we check if there are more than one(or more - this is modified according to current
				# situation) regular expressions mathing this pattern: customer2-policy2 INCR 2 - we look for "customer2" and "2"(after INCR) so we are sure that we count only same customer with the
				# same exit code.
				log.append(
					"\n" + "Log creation time: " + log_time + "\n" + "Following alerts will be aggregated by exit code: " + exit_code + " for customer: " + customer + "\n")  # adding error code aggregation header to the log file
				for tmp in data:
					if len(re.findall(
							r"\b%s\S*\b\s\S*\s%d\s\S*\s\S*\s\S*\s\d*\s*\n" % (re.escape(customer), int(exit_code)),
							tmp)):  # we look for strings that match the pattern we use to mark strings for error aggregation
						log.append(tmp.rstrip() + "\n")
			else:
				result.append(item.rstrip())
	with open(log_file_path, mode="a") as logfile:  # writing log to the log file
		for _ in list(OrderedDict.fromkeys(log)):
			logfile.writelines(_)
	with open(output_file_path, mode="a") as outfile:
		for _ in result:
			outfile.writelines(_)


if check_env():
	#sort_by_code(sort_by_policy(get_failed_strings(r"C:\Users\yaizk\PycharmProjects\coursera_l2\Practice Python\Testing\testing_aggregation.txt")))
	sort_by_code(sort_by_policy(get_failed_strings(data_file_path)))
	for item in logs:
		log_cleanup(item)
